---
title: 'Semi-Supervised Learning: the Case When Unlabeled Data is Equally Useful'
abstract: 'Semi-supervised learning algorithms attempt to take advantage of relatively
  inexpensive unlabeled data to improve learning performance. In this work, we consider
  statistical models where the data distributions can be characterized by continuous
  parameters. We show that under certain conditions on the distribution, unlabeled
  data is equally useful as labeled date in terms of learning rate. Specifically,
  let $n, m$ be the number of labeled and unlabeled data, respectively. It is shown
  that the learning rate of semi-supervised learning scales as $O(1/n)$ if $m\sim
  n$, and scales as $O(1/n^{1+\gamma})$ if $m\sim n^{1+\gamma}$ for some $\gamma>0$,
  whereas the learning rate of supervised learning scales as $O(1/n)$.  (Note: this version contains an error in the proof of Lemma~2. A corrected version is available on arXiv)'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhu20b
month: 0
tex_title: 'Semi-Supervised Learning: the Case When Unlabeled Data is Equally Useful'
firstpage: 709
lastpage: 718
page: 709-718
order: 709
cycles: false
bibtex_author: Zhu, Jingge
author:
- given: Jingge
  family: Zhu
date: 2020-08-27
address: 
container-title: Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence
  (UAI)
volume: '124'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 8
  - 27
pdf: http://proceedings.mlr.press/v124/zhu20b/zhu20b.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v124/zhu20b/zhu20b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
